{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import imageio\n",
    "\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from src.model.TransferModel import get_style_model_and_losses\n",
    "from src.dataset.dataset import ImgDataset\n",
    "from src.utils.other import *\n",
    "from src.utils.plotting import canvas2rgb_array\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grid of annie, kennen, mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "imsize = 256 if torch.cuda.is_available() else 128  # use small size if no gpu\n",
    "\n",
    "style_dataset = ImgDataset(\"../../data/traditional_style_images/\", imsize, device)\n",
    "style_loader = DataLoader(style_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "outfolder = \"../../results/evolution/traditional_style_images_256/\"\n",
    "os.system(\"mkdir -p \" + str(outfolder))\n",
    "\n",
    "# the rest of this cell from\n",
    "# https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "def imshow(tensor, ax, title=None, fontsize=None):\n",
    "    unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "    image = tensor.cpu().clone() # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze()      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    ax.imshow(image)\n",
    "    if title is not None:\n",
    "        if fontsize is None:\n",
    "            ax.set_title(title)\n",
    "        else:\n",
    "            ax.set_title(title, fontsize=fontsize)\n",
    "    #ax.pause(0.001) # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "# we modified this method to allow us to save the image during optimization\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1000000, content_weight=1, \n",
    "                       content_layers=['conv_4'],\n",
    "                       style_layers=['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5'],\n",
    "                       do_print=True, device=\"cpu\", save_interval=10, fpath=None):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    if do_print:\n",
    "        print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img, device,\n",
    "        content_layers, style_layers)\n",
    "\n",
    "    # We want to optimize the input and not the model parameters so we\n",
    "    # update all the requires_grad fields accordingly\n",
    "    input_img.requires_grad_(True)\n",
    "    model.requires_grad_(False)\n",
    "\n",
    "    optimizer = optim.LBFGS([input_img])\n",
    "    \n",
    "    unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "    \n",
    "    if do_print:\n",
    "        print('Optimizing..')\n",
    "    run = [0]\n",
    "    count = 0\n",
    "    images = []\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    image = input_img.clone().clamp_(0, 1).cpu()\n",
    "    image = image.squeeze()\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    plt.close()\n",
    "    data = canvas2rgb_array(fig.canvas)\n",
    "    images.append(data)\n",
    "\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            with torch.no_grad():\n",
    "                input_img.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                if do_print:\n",
    "                    print(\"run {}:\".format(run))\n",
    "                    print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                        style_score.item(), content_score.item()))\n",
    "                    print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        count += 10\n",
    "        \n",
    "        if count % save_interval == 0:\n",
    "            #fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "            #fig = plt.figure(figsize=(5, 5))\n",
    "            fig = plt.figure(figsize=(3, 3))\n",
    "            ax = plt.gca()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "            image = input_img.clone().clamp_(0, 1).cpu()\n",
    "            image = image.squeeze()\n",
    "            image = unloader(image)\n",
    "            plt.imshow(image)\n",
    "            plt.close()\n",
    "            data = canvas2rgb_array(fig.canvas)\n",
    "            images.append(data)\n",
    "    imageio.mimsave(fpath + \".gif\", images, duration=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 310.367218 Content Loss: 20.954500\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 90.559570 Content Loss: 24.542795\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 41.321701 Content Loss: 25.874041\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 25.920315 Content Loss: 25.775944\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 18.034121 Content Loss: 25.633121\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 13.496673 Content Loss: 25.218895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps=300\n",
    "style_weight=1000000\n",
    "content_weight=1\n",
    "fontsize= 15\n",
    "interval=10\n",
    "\n",
    "# get annie data\n",
    "annie_content = style_dataset.load_image(\"../../data/Annie_2-2.jpg\", device)\n",
    "for i, annie_style in enumerate(style_loader):\n",
    "    if i == 5:\n",
    "        break      \n",
    "annie_style = annie_style.squeeze(0)\n",
    "\n",
    "# annie\n",
    "run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                                  annie_content, annie_style, annie_content.clone(), num_steps=steps,\n",
    "                                  style_weight=style_weight, content_weight=content_weight,\n",
    "                                  do_print=True, device=device, save_interval=interval,\n",
    "                                  fpath=outfolder + \"annie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1896.307617 Content Loss: 48.923653\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 379.989746 Content Loss: 53.387779\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 161.407211 Content Loss: 53.977608\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 84.243683 Content Loss: 54.779457\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 48.499767 Content Loss: 55.218140\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 31.059223 Content Loss: 55.247589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps=300\n",
    "style_weight=1000000\n",
    "content_weight=1\n",
    "fontsize= 15\n",
    "\n",
    "interval=10\n",
    "\n",
    "# get kennen data\n",
    "kennen_content = style_dataset.load_image(\"../../data/Kennen_3.jpg\", device)\n",
    "for i, kennen_style in enumerate(style_loader):\n",
    "    if i == 6:\n",
    "        break\n",
    "kennen_style = kennen_style.squeeze(0)\n",
    "\n",
    "\n",
    "# kennen\n",
    "run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                                   kennen_content, kennen_style, kennen_content.clone(), num_steps=steps,\n",
    "                                   style_weight=style_weight, content_weight=content_weight,\n",
    "                                   do_print=True, device=device, save_interval=interval,\n",
    "                                   fpath=outfolder + \"kennen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 161.376236 Content Loss: 20.096439\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 53.316444 Content Loss: 19.499821\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 33.284462 Content Loss: 18.431530\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 24.201576 Content Loss: 17.339361\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 18.293856 Content Loss: 16.483480\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 13.727413 Content Loss: 15.889206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps=300\n",
    "style_weight=1000000\n",
    "content_weight=1\n",
    "fontsize= 15\n",
    "\n",
    "interval=10\n",
    "\n",
    "# get mf data\n",
    "mf_path = \"../../data/missfortuneloadscreen_17.skins_missfortune_skin15_chromas-300x545.jpg\"\n",
    "mf_content = style_dataset.load_image(mf_path, device)\n",
    "for i, mf_style in enumerate(style_loader):\n",
    "    pass\n",
    "mf_style = mf_style.squeeze(0)\n",
    "\n",
    "# mf\n",
    "run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                               mf_content, mf_style, mf_content.clone(), num_steps=steps,\n",
    "                               style_weight=style_weight, content_weight=content_weight,\n",
    "                               do_print=True, device=device, save_interval=interval,\n",
    "                               fpath=outfolder + \"mf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 288.739349 Content Loss: 40.792854\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 70.445267 Content Loss: 42.472816\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 30.702358 Content Loss: 42.772461\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 19.018688 Content Loss: 42.511086\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 11.512776 Content Loss: 42.204685\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 8.646844 Content Loss: 41.623611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps=300\n",
    "style_weight=1000000\n",
    "content_weight=1\n",
    "fontsize= 15\n",
    "\n",
    "interval=10\n",
    "\n",
    "# get mf data\n",
    "jhin_path = \"../../data/jhin-bloodmoon-300x545.jpg\"\n",
    "jhin_content = style_dataset.load_image(jhin_path, device)\n",
    "for i, jhin_style in enumerate(style_loader):\n",
    "    if i == 1:\n",
    "        break\n",
    "jhin_style = jhin_style.squeeze(0)\n",
    "\n",
    "# mf\n",
    "run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                               jhin_content, jhin_style, jhin_content.clone(), num_steps=steps,\n",
    "                               style_weight=style_weight, content_weight=content_weight,\n",
    "                               do_print=True, device=device, save_interval=interval,\n",
    "                               fpath=outfolder + \"jhin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 37.206230 Content Loss: 31.795567\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 11.350952 Content Loss: 28.921564\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 5.424634 Content Loss: 24.815208\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 3.278734 Content Loss: 21.240761\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 2.331804 Content Loss: 18.814152\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 1.887944 Content Loss: 17.154497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps=300\n",
    "style_weight=1000000\n",
    "content_weight=1\n",
    "fontsize= 15\n",
    "\n",
    "interval=10\n",
    "\n",
    "# get mf data\n",
    "img_path = \"../../data/Garen_5-300x545.jpg\"\n",
    "img_content = style_dataset.load_image(img_path, device)\n",
    "for i, img_style in enumerate(style_loader):\n",
    "    if i == 0:\n",
    "        break\n",
    "img_style = img_style.squeeze(0)\n",
    "\n",
    "# mf\n",
    "run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                               img_content, img_style, img_content.clone(), num_steps=steps,\n",
    "                               style_weight=style_weight, content_weight=content_weight,\n",
    "                               do_print=True, device=device, save_interval=interval,\n",
    "                               fpath=outfolder + \"garen\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 227.244949 Content Loss: 21.349361\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 93.222656 Content Loss: 22.098766\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 52.769402 Content Loss: 22.247578\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 34.929718 Content Loss: 21.496502\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 23.799997 Content Loss: 20.737022\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 15.932960 Content Loss: 20.188984\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps=300\n",
    "style_weight=1000000\n",
    "content_weight=1\n",
    "fontsize= 15\n",
    "\n",
    "interval=10\n",
    "\n",
    "# get mf data\n",
    "img_path = \"../../data/Zilean_4-300x545.jpg\"\n",
    "img_content = style_dataset.load_image(img_path, device)\n",
    "for i, img_style in enumerate(style_loader):\n",
    "    if i == 3:\n",
    "        break\n",
    "img_style = img_style.squeeze(0)\n",
    "\n",
    "# mf\n",
    "run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                               img_content, img_style, img_content.clone(), num_steps=steps,\n",
    "                               style_weight=style_weight, content_weight=content_weight,\n",
    "                               do_print=True, device=device, save_interval=interval,\n",
    "                               fpath=outfolder + \"zilean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
